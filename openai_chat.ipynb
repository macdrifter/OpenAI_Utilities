{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Pre-requisites for using this notebook\n",
    "\n",
    "1. Pandas\n",
    "2. [OpenAI](https://pypi.org/project/openai/)\n",
    "3. [Tiktoken](https://pypi.org/project/tiktoken/)\n",
    "4. [Dotenv](https://pypi.org/project/python-dotenv/)\n",
    "\n",
    "Features\n",
    "\n",
    "1. Load the [Functions](#chatgpt-functions)\n",
    "2. [One Shot Question](#one-shot-question)\n",
    "3. [List All Conversations History](#get-entire-chat-history) to Retrieve ID\n",
    "4. [Search Conversation History](#search-history)\n",
    "5.  [Continue Conversation by ID](#continue-chat-by-id)\n",
    "6. [Summarize Conversation by ID](#summarize-conversation-by-id)\n",
    "7. [Create Prompt Archive](#create-steering-sources-csv)\n",
    "8. Get [Random Prompt](#random-steering-prompt)\n",
    "9. [Check Token Size](#prompt-size-check) for a prompt string\n",
    "\n",
    "TODO\n",
    "\n",
    "- [ x ] Saved Persona List\n",
    "- [ x ] Random Prompt\n",
    "- [   ] New System Prompt\n",
    "- Generate detailed description for DALL-E\n",
    "- [ x ] Check Token size for History\n",
    "- Update Continue Conversation to populate token size\n",
    "- Warning when token size approaches limit\n",
    "- Add Summary to Chat History ID\n",
    "- Test Prompting for Sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('CHATGPTAPI')\n",
    "openai.organization = os.getenv('OPENAPI_ORG_ID')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Empty Log Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['conversation_id', 'message', 'response','raw_message','raw_response','conversation_message','date']\n",
    "chat_df = pd.DataFrame(columns=columns)\n",
    "chat_df.to_pickle('chat_log.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Functions\n",
    "\n",
    "Prerequisites:\n",
    "- OpenAI library\n",
    "  - OpenAI ChatGPT API Key\n",
    "  - OpenAI Organization ID\n",
    "- dotenv library\n",
    "- Pandas\n",
    "- A pandas data frame to use as the log\n",
    "\n",
    "The Pandas data frame log should have the following series:\n",
    "\n",
    "`'conversation_id', 'message', 'response','raw_message','raw_response','conversation_message','date'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('CHATGPTAPI')\n",
    "openai.organization = os.getenv('OPENAPI_ORG_ID')\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Holds the foundational instructions for the AI. The role should be \"system\"\n",
    "steering_prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides direct clear responses with short explanations.\"}\n",
    "    ]\n",
    "STEERING_PROMPT = steering_prompt\n",
    "\n",
    "def get_response(message_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Function to call the ChatCompletion (ChatGPT) endpoint from OpenAI. Accepts a dict of messages.\n",
    "    \n",
    "    Stream is always set to false so we wait for a complete response instead of paging results.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=message_dict,\n",
    "        temperature=0,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.to_dict()\n",
    "\n",
    "# Get the most recent row for a chat_id value\n",
    "def get_last_conversation(chat_id:str, chat_history:pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Function to test if a data frame log contains a specific chat ID. If it does it returns the most recent conversation list\n",
    "    \"\"\"\n",
    "    verified_records = chat_history[chat_history['conversation_id'] == chat_id]\n",
    "    if verified_records.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return verified_records.sort_values(by='date').iloc[-1]['conversation_message']\n",
    "\n",
    "# Function  to append the new query to chat instruction list\n",
    "\n",
    "def format_chat_message(message_history: list, new_prompt: str) -> list:\n",
    "    \"\"\"\n",
    "    Function to append a new prompt with a list of previous prompts. New prompt is always appended with the `user` role.\n",
    "    \"\"\"\n",
    "    message_history.append({\"role\": \"user\", \"content\": new_prompt})\n",
    "    return message_history\n",
    "\n",
    "    \n",
    "def append_chat_df(log_dataframe:pd.DataFrame,\n",
    "                   messages:list,\n",
    "                   response:dict,\n",
    "                   conv_id:str =None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Appends chat messages to a dataframe for the purpose of logging chat sessions.\n",
    "    \n",
    "    Parameters:\n",
    "    log_dataframe: Pandas data frame that holds the log of messages\n",
    "    messages: List that contains all prompts, including steering prompt\n",
    "    response: Dictionary holding the OpenAI response\n",
    "    conv_id: Conversation ID used to aggregrate chat into a thread. The default value is based on time stamp string.\n",
    "    \n",
    "    Returns:\n",
    "    Data frame containing all previous log entries plus new entry from `messages` and `response`\n",
    "    \"\"\"\n",
    "    if conv_id is None:\n",
    "        conv_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # The newest prompt will be the last itme in the list.\n",
    "    \n",
    "    new_prompt = messages[-1]['content']\n",
    "    # The response will be the message.content value from choices\n",
    "    new_response = response['choices'][0]['message']['content']\n",
    "    # make a copy of the messages so we can append to it\n",
    "    conversation = messages.copy()\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": new_response})\n",
    "    # Create a new data frame to hold just the new log entry. Then concatenate it with the passed data frame.\n",
    "    new_row = pd.DataFrame({'conversation_id':[conv_id], 'message': [new_prompt], 'response': [new_response],\n",
    "                            'conversation_message': [conversation],\n",
    "                            'raw_message':[messages], 'raw_response':[response], \n",
    "                            'date':[time.time()]})\n",
    "    log_dataframe = pd.concat([log_dataframe, new_row], ignore_index=True)\n",
    "    return log_dataframe\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def num_tokens_from_messages(messages:list, model=\"gpt-3.5-turbo-0301\") ->int:\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -> One Shot Question\n",
    "\n",
    "Ask a stand alone question and get a succinct response. Markdown is rendered in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "new_prompt = r'''Describe how I can search Chembl using RDKit to find moleules that are schedule 1 substances\n",
    "\n",
    "'''\n",
    "\n",
    "chat_prompt = format_chat_message(steering_prompt, new_prompt)\n",
    "\n",
    "# Print out the new question that we just appended\n",
    "print(chat_prompt[-1]['content'])\n",
    "\n",
    "# Call the API with the new chat prompt\n",
    "new_chat = get_response(chat_prompt)\n",
    "\n",
    "# Append with specified conversation ID\n",
    "df_chat = append_chat_df(df_chat,chat_prompt,new_chat)\n",
    "\n",
    "chat_id = df_chat.sort_values(by='date').iloc[-1]['conversation_id']\n",
    "print(f'Chat ID = {chat_id}')\n",
    "# Print the formatted response\n",
    "display(Markdown(new_chat['choices'][0]['message']['content']))\n",
    "#print(new_chat['choices'][0]['message']['content'])\n",
    "\n",
    "# Persist our work\n",
    "df_chat.to_pickle('chat_log.pkl')\n",
    "df_chat.to_excel('df_chat.xlsx')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "new_prompt = r'''How are controlled substances harmonized when transporting between countries?\n",
    "'''\n",
    "override_steering_prompt = r'''You are a PhD organic chemist. You work in the pharmaceutical industry as a research scientist. You have expertise in cheminformatics.\n",
    "'''\n",
    "\n",
    "#steering_prompt = STEERING_PROMPT.append({\"role\": \"system\", \"content\": f'{override_steering_prompt}'})\n",
    "\n",
    "chat_prompt = format_chat_message(steering_prompt, new_prompt)\n",
    "\n",
    "# Print out the new question that we just appended\n",
    "print(chat_prompt[-1]['content'])\n",
    "\n",
    "# Call the API with the new chat prompt\n",
    "new_chat = get_response(chat_prompt)\n",
    "\n",
    "# Append with specified conversation ID\n",
    "df_chat = append_chat_df(df_chat,chat_prompt,new_chat)\n",
    "\n",
    "chat_id = df_chat.sort_values(by='date').iloc[-1]['conversation_id']\n",
    "print(f'Chat ID = {chat_id}')\n",
    "# Print the formatted response\n",
    "display(Markdown(new_chat['choices'][0]['message']['content']))\n",
    "#print(new_chat['choices'][0]['message']['content'])\n",
    "\n",
    "# Persist our work\n",
    "df_chat.to_pickle('chat_log.pkl')\n",
    "df_chat.to_excel('df_chat.xlsx')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Conversation\n",
    "\n",
    "Have a multi-question chat with ChatGPT using a `conversation_id` from the chat log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case\n",
    "\n",
    "Use a previous conversation as a test case for multi-response chat. `historic_chat` should provide context for ChatGPT to provide consistent follow up responses. It also allows for references to previous responses without quoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "historic_chat = [{\"role\": \"user\", \"content\": r'I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is \"I am in Sorrento Italy and I want to visit pretty places to relax.\"'},\n",
    " {\"role\": \"assistant\", \"content\": r'Yes, there are several wineries near Positano that you can visit to taste local wines and learn about the winemaking process. Here are a few options:\\n\\nCantine Marisa Cuomo: This winery is located in the town of Furore, just a short drive from Positano. It\\'s known for its unique wines, which are made using grapes grown on steep terraces overlooking the sea. The winery also offers tours and tastings, as well as a restaurant with stunning views of the Amalfi Coast.\\nMontevetrano Winery: This winery is located in the town of San Cipriano Picentino, about an hour\\'s drive from Positano. It\\'s known for its high-quality wines, which are made using a blend of traditional Italian and French winemaking techniques. The winery also offers tours and tastings, as well as a beautiful outdoor terrace where you can enjoy a glass of wine while taking in the views of the vineyards.\\nLe Vigne di Raito: This winery is located in the town of Raito, just a short drive from Positano. It\\'s known for its organic wines, which are made using traditional winemaking techniques and local grape varieties. The winery also offers tours and tastings, as well as a beautiful terrace where you can relax and enjoy the views of the Mediterranean Sea.\\nAll of these wineries offer a unique and memorable experience for wine lovers, and they\\'re all located within a reasonable driving distance from Positano.\\n'},\n",
    " {\"role\": \"user\", \"content\": r'Suggest places within walking distance of Sorrento town center'},\n",
    " {\"role\": \"assistant\", \"content\": r'Certainly! Here are some suggestions for places within walking distance of Sorrento town center:\\n\\nPiazza Tasso: This is the main square in Sorrento and is located right in the town center. Here you can find plenty of cafes, restaurants, and shops, as well as beautiful architecture and fountains. It\\'s a great place to relax and people-watch.\\nMarina Grande: This small fishing village is located just a short walk from Sorrento town center and is known for its colorful boats, charming restaurants, and beautiful sea views. Take a stroll along the harbor or enjoy a seafood meal at one of the local restaurants.\\nVilla Comunale: This public park is located on a cliff overlooking the sea, just a short walk from Sorrento town center. Here you can enjoy stunning views of the Gulf of Naples, as well as beautiful gardens and a small outdoor theater.\\nMuseo Correale di Terranova: This museum is located in a beautiful villa just a short walk from Sorrento town center and features a collection of art and decorative objects from the 17th to the 19th centuries. It\\'s a great place to explore if you\\'re interested in history and art.\\nSorrento Cathedral: This beautiful cathedral is located in the heart of Sorrento town center and is known for its stunning frescoes and baroque architecture. It\\'s a great place to visit if you\\'re interested in art and architecture.\\n'},\n",
    " {\"role\": \"user\", \"content\": r'Ok, I\\'d like to visit the Marina from Sorrento center. Please provide walking directions'},\n",
    " {\"role\": \"assistant\", \"content\": r'Sure! Walking from Sorrento town center to Marina Grande is quite easy and will take you approximately 10-15 minutes. Here are the walking directions:\\n\\nHead south on Corso Italia, the main street in Sorrento town center, towards Piazza Tasso.\\nOnce you reach Piazza Tasso, continue straight ahead on Via Luigi de Maio. You will pass by a few shops and restaurants on your left and right.\\nKeep walking straight until you reach the end of Via Luigi de Maio, and then turn left onto Via Marina Grande. This street will lead you directly to the marina.\\nFollow Via Marina Grande all the way down to the seafront, where you will find the small fishing village of Marina Grande.\\nAlong the way, you will pass by some beautiful views of the Gulf of Naples and the Sorrento coastline. Once you arrive at Marina Grande, you can explore the harbor, take a stroll along the beach, or enjoy a meal at one of the many seafood restaurants.\\n'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.displayfrom  import Markdown\n",
    "import html\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "new_prompt = 'Is there anywhere I can get gelato on the way to the Marina?'\n",
    "chat_prompt = format_chat_message(historic_chat, new_prompt)\n",
    "\n",
    "# Call the API with the new chat prompt\n",
    "new_chat = get_response(chat_prompt)\n",
    "\n",
    "# Append with specified conversation ID\n",
    "df_chat = append_chat_df(df_chat,chat_prompt,new_chat)\n",
    "\n",
    "chat_id = df_chat.sort_values(by='date').iloc[-1]['conversation_id']\n",
    "print(f'Chat ID = {chat_id}')\n",
    "# Print the response\n",
    "display(Markdown(html.unescape(new_chat['choices'][0]['message']['content'])))\n",
    "\n",
    "# Persist our work\n",
    "\n",
    "\n",
    "\n",
    "df_chat.to_pickle('chat_log.pkl')\n",
    "df_chat.to_excel('df_chat.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> Continue Chat by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data frame log\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "new_prompt = \"\"\"I don't think that's correct. The `filter` function accepts parameter assignments, not a dictionary.\n",
    "\"\"\"\n",
    "\n",
    "# Enter a conversation ID to instruct ChatGPT about the conversation history.\n",
    "conversation_id = '20230309082116'\n",
    "\n",
    "last_convo = get_last_conversation(conversation_id,df_chat)\n",
    "if  last_convo == None:\n",
    "        chat_prompt = format_chat_message(steering_prompt, new_prompt)\n",
    "        conversation_id = None\n",
    "else:\n",
    "        chat_prompt = format_chat_message(last_convo, new_prompt)\n",
    "\n",
    "# Print out the new question that we just appended\n",
    "print(chat_prompt[-1]['content'])\n",
    "\n",
    "# Call the API with the new chat prompt\n",
    "new_chat = get_response(chat_prompt)\n",
    "\n",
    "# Append with specified conversation ID\n",
    "df_chat = append_chat_df(df_chat,chat_prompt,new_chat, conversation_id)\n",
    "\n",
    "# Print the response\n",
    "display(Markdown(new_chat['choices'][0]['message']['content'].replace('\\\\n', '\\n')))\n",
    "\n",
    "# Persist our work\n",
    "df_chat.to_pickle('chat_log.pkl')\n",
    "df_chat.to_excel('df_chat.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search and View Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Entire Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Questions Asked\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "\n",
    "def display_questions(chat_history:pd.DataFrame):\n",
    "    for index, row in chat_history.iterrows():\n",
    "        display(Markdown(f'ID: {row[\"conversation_id\"]} --> {row[\"message\"]}'.replace('\\\\n', '\\n')))\n",
    "\n",
    "\n",
    "display_questions(df_chat)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search History for String\n",
    "import pandas as pd\n",
    "\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "\n",
    "def search_questions(search_phrase:str, chat_history:pd.DataFrame):\n",
    "    search_mask = chat_history['message'].str.contains(search_phrase, case=False)\n",
    "    matches = chat_history[search_mask]\n",
    "    for index, row in matches.iterrows():\n",
    "        display(Markdown(f'ID: {row[\"conversation_id\"]} --> {row[\"message\"]}'.replace('\\\\n', '\\n')))\n",
    "\n",
    "my_search = 'pandas'\n",
    "search_questions(my_search, df_chat)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Conversation by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "chat_history_id = '20230305074049'\n",
    "\n",
    "last_convo = get_last_conversation(chat_history_id,df_chat)\n",
    "for response in last_convo:\n",
    "    display(Markdown(f'{response[\"role\"]}: {response[\"content\"]}'.replace('\\\\n', '\\n')))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Conversation by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "df_chat = pd.read_pickle('chat_log.pkl')\n",
    "# 20230308100018\n",
    "# 20230305185226\n",
    "chat_history_id = '20230305185226'\n",
    "summarize_prompt = 'Given the above conversation, please provide a short bullet list summary of the discussion with each bullet point no longer than 10 words. Provide the very first user question asked as the introduction to the bullet list'\n",
    "last_convo = get_last_conversation(chat_history_id,df_chat)\n",
    "chat_prompt = format_chat_message(last_convo, summarize_prompt)\n",
    "new_chat = get_response(chat_prompt)\n",
    "display(Markdown(new_chat['choices'][0]['message']['content'].replace('\\\\n', '\\n')))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona and Steering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Steering Sources CSV\n",
    "\n",
    "This only needs to be executed once. After creation new entries can be added manually or through a DF and resaved to CSV. The CSV file will remain as the permanent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty DF\n",
    "import pandas as pd\n",
    "\n",
    "columns = ['description', 'role', 'prompt', 'category', 'tags', 'actor', 'source']\n",
    "chat_steering_df = pd.DataFrame(columns=columns)\n",
    "chat_steering_df.to_pickle('steering_prompts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "awesome_prompt_url = 'https://raw.githubusercontent.com/f/awesome-chatgpt-prompts/main/prompts.csv'\n",
    "\n",
    "awesome_df = pd.read_csv(awesome_prompt_url, usecols=[0, 1], on_bad_lines='warn')\n",
    "awesome_df.columns=['actor', 'prompt']\n",
    "chat_steering_df = pd.concat([chat_steering_df, awesome_df], ignore_index=True)\n",
    "chat_steering_df.loc[:, 'source'] = 'https://raw.githubusercontent.com/f/awesome-chatgpt-prompts/main/prompts.csv'\n",
    "chat_steering_df.to_csv('steering_prompts.csv') # save as CSV so we can manually edit and add items as needed\n",
    "\n",
    "chat_steering_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Prompt CSV to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_steering_df = pd.read_csv(\"steering_prompts.csv\")\n",
    "chat_steering_df = chat_steering_df.astype('str')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Steering Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_steering_df = pd.read_csv(\"steering_prompts.csv\")\n",
    "search_field = 'prompt'\n",
    "search_word = 'excel'\n",
    "exclude_category = 'template' #Template prompts have fields to fill in with additional information.\n",
    "chat_steering_df = chat_steering_df.fillna('') #Fill with empty strings so we can conveniently search. Better to update and save the DF but this works too.\n",
    "result_df = chat_steering_df[chat_steering_df[search_field].str.contains(search_word) & ~chat_steering_df['category'].str.contains(exclude_category)]\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Steering Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "exclude_term = 'template'\n",
    "not_template_df = chat_steering_df.query('category != @exclude_term')\n",
    "\n",
    "random_row = not_template_df.sample(n=1)\n",
    "text = random_row[\"prompt\"].values[0]\n",
    "display(Markdown(f'{text}'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Size Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "conversation_id = '20230308100018'\n",
    "\n",
    "last_convo = get_last_conversation(conversation_id,df_chat)\n",
    "print(num_tokens_from_messages(last_convo))\n",
    "print(encoding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "Jupyter.notebook.execute_cells([2])",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.execute_cells([2])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "def execute_cell(cell_index):\n",
    "    shell = get_ipython()\n",
    "    cells = shell.user_ns['get_ipython']().user_ns['In']\n",
    "    result = shell.run_cell(cells[cell_index])\n",
    "    return result\n",
    "execute_cell(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\nvar output = null;\nvar callback = function (msg) {\n    if (msg.msg_type === \"execute_result\") {\n        output = msg.content.data[\"text/plain\"];\n    }\n};\nJupyter.notebook.execute_cells_by_id(['abc1234567'], {'output': callback});\noutput;\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "# Replace 'my_unique_id' with the ID of the cell you want to execute\n",
    "cell_id = 'abc1234567'\n",
    "\n",
    "# Construct the JavaScript code to execute the cell\n",
    "js_code = f\"\"\"\n",
    "var output = null;\n",
    "var callback = function (msg) {{\n",
    "    if (msg.msg_type === \"execute_result\") {{\n",
    "        output = msg.content.data[\"text/plain\"];\n",
    "    }}\n",
    "}};\n",
    "Jupyter.notebook.execute_cells_by_id(['{cell_id}'], {{'output': callback}});\n",
    "output;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the JavaScript code\n",
    "results = Javascript(js_code)\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_ipython().parent_header['msg_id'])\n",
    "print(\"Hello\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "chat = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides direct clear responses.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How large is your model?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "#print(response['choices'][0]['message']['content'])\n",
    "for response in chat:\n",
    "    print(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Response Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_block = json.loads(str(df_chat['raw_response'][0]['choices'][0]))\n",
    "print(str(df_chat['raw_response'][0]['choices'][0]['message']['content']))\n",
    "print(df_chat['raw_message'][0][-1]['content'])\n",
    "print(new_chat['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Types and Docs\n",
    "\n",
    "Pretty handy in split view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IPython.core.display.Javascript'>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_data', '_data_and_metadata', '_read_flags', '_repr_javascript_', '_show_mem_addr', 'css', 'data', 'filename', 'lib', 'metadata', 'reload', 'url']\n"
     ]
    }
   ],
   "source": [
    "print(type(results))\n",
    "print(dir(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_chat['messages'][0])\n",
    "# Append the response from this query as a new element.\n",
    "df_chat['messages'][0].append(str(df_chat['response'][0]['choices'][0]))\n",
    "print(df_chat['messages'][0])\n",
    "\n",
    "#json_message = json.loads(str(df_chat['response'][1]['choices'][0]))\n",
    "#print(json_block['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "Not functional and not a priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the chatbot's response\n",
    "def get_response(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=MODEL,\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Start the chat interface\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    transcript_file.write(\"You: \" + user_input + \"\\n\")\n",
    "    prompt = \"Me: \" + user_input + \"\\nYou:\"\n",
    "    response = get_response(prompt)\n",
    "    print(\"Me:\", response)\n",
    "    pyperclip.copy(response) # copy the response to clipboard\n",
    "    transcript_file.write(\"Me: \" + response + \"\\n\")\n",
    "\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "# Close the transcript file\n",
    "transcript_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7fd4aecff4b6388b3a3c53558fe8a4a879c5fa188c3468c20e0fab9574062ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
